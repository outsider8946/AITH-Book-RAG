Текущий папйлайн:
1. Получаем запрос пользователя
2. С помощью LLM извлекаем сущности (персонажи, предикаты, места) из запроса.
3. Проводим нормализацию названий сущностей.
4. В заранее построенном KG делаем поиск по нодам, извлеченным пользователями
5. Для извлеченных нод берем все существуюшие связи и строим векторную БД по описаниями связей.
6. С помощью эмбедера делаем векторный поиск для предиката, извлеченного из запроса пользователя.
7. Выбираем топ 3 связи по схожести с предикатом.
8. С помощью LLM формируем ответ на вопрос пользователя.

(пайплайн будет еще улучшаться)

Компоненты:
 1. KG - граф знаний, построенный с помощью LLM и neo4j (делем запрос к нему после получения субграфа пользователя)
 2. Frontend - чат с RAG. Отправляем в пайплайн рага запрос пользователя, написанный в чате.
 3. Backend - звено между RAG и frontend.
 4. RAG - пайплайн по обработке запроса. 

Тестовый набор - 30 придуманных вопросов по тематике романа "Граф Монте-Кристо".
Структура тестового набора включает: вопрос, правильный ответ, ответ пайплайна, контекст пайплайна.

Метрики на тестовом наборе:
'faithfulness': 0.2500 
'context_precision': 0.2963 
'context_recall': 0.2500
